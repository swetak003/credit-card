{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAQAxE_8gHcJ"
      },
      "source": [
        "# Credit Card Fraud Detection Capstone Project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4R0eNc5FdzQt"
      },
      "source": [
        "\\### Contributed By : Satinder Malik\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CT7xxUb3dzWr"
      },
      "source": [
        "## Problem Statement:\n",
        "The problem statement chosen for this project is to predict fraudulent credit card transactions with the help of machine learning models. Due to the steep increase in banking frauds, it is the need of the hour to detect these fraudulent transactions in time in order to help consumers as well as banks, who are losing their credit worth each day.\n",
        "Every fraudulent credit card transaction that occur is a direct financial loss to the bank as the bank is responsible for the fraud transactions as well it also affects the overall customer satisfaction adversely.    \n",
        "**The aim of this project is to identify and predict fraudulent credit card transactions using machine learning models.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "id": "tQ3CL-BugHcO",
        "outputId": "86b95ab4-9a99-4f06-d33c-555b04596d0c",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# Importing python libraries :\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "\n",
        "# import machine learning and stats libraries:\n",
        "from scipy import stats\n",
        "from scipy.stats import norm, skew\n",
        "from scipy.special import boxcox1p\n",
        "from scipy.stats import boxcox_normmax\n",
        "\n",
        "import sklearn\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "from sklearn.metrics import average_precision_score, precision_recall_curve\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "\n",
        "from sklearn.linear_model import Ridge, Lasso, LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import xgboost as xgb\n",
        "from xgboost import XGBClassifier\n",
        "from xgboost import plot_importance\n",
        "# Import:\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "#install scikit-optimize\n",
        "#!pip install scikit-optimize\n",
        "from skopt import BayesSearchCV\n",
        "\n",
        "\n",
        "# To ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWiXuaqRgHcY"
      },
      "source": [
        "## EDA and Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oLCvT6qtqKF3"
      },
      "outputs": [],
      "source": [
        "#To read csv File from locally stored file\n",
        "df_credit = pd.read_csv('creditcard.csv')\n",
        "df_credit.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "rBzcdnQl6Cex",
        "outputId": "bc62dd11-f2ba-4f76-a074-635b20b744fe"
      },
      "outputs": [],
      "source": [
        "# Examining the dataset imported:\n",
        "df_credit.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "u_cTZWmY6EnI",
        "outputId": "b5b7e6f7-e5fd-47c8-d44e-343d70c209c1"
      },
      "outputs": [],
      "source": [
        "# Lets check the numeric distribution of the data:\n",
        "df_credit.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "id": "op7Rr-yg6OjY",
        "outputId": "c0bceea5-5b22-4d81-c2c9-67654af4181d"
      },
      "outputs": [],
      "source": [
        "#Examining the data frame for the shape, datatypes, NUlls etc\n",
        "df_credit.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "id": "PwhBZ-A30ifF",
        "outputId": "9d7b3315-5984-4356-876a-f8f100711c8b"
      },
      "outputs": [],
      "source": [
        "#Check the fraud/Non_Fraud related records\n",
        "df_credit['Class'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "id": "n2WePKuipXMH",
        "outputId": "7b0e485a-0388-47f6-e917-cde4c40b1770"
      },
      "outputs": [],
      "source": [
        "#find % values of class\n",
        "(df_credit.groupby('Class')['Class'].count()/df_credit['Class'].count()) *100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "id": "4Z4WXgEVAVoL",
        "outputId": "335934b6-6ff7-4aff-df1e-084903fe26b5"
      },
      "outputs": [],
      "source": [
        "#check if any null values\n",
        "df_credit.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "id": "F642io__gHch",
        "outputId": "8d6b4b2e-9e68-42bf-aa6f-c3baf9d0a4c8"
      },
      "outputs": [],
      "source": [
        "#observe the different feature type present in the data\n",
        "#lets check data types of the features\n",
        "df_credit.dtypes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6H0Bt9q7ev7P",
        "outputId": "90cba092-502e-4d64-c9d1-e54f7b61735c"
      },
      "outputs": [],
      "source": [
        "# Finding the initial full correlation in the dataset:\n",
        "\n",
        "# correlation matrix\n",
        "cor = df_credit.corr()\n",
        "cor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6nasAIhuew1H",
        "outputId": "2e98c3b7-a7a2-4a8f-be67-a34477463648"
      },
      "outputs": [],
      "source": [
        "# plotting correlations on a heatmap\n",
        "\n",
        "# figure size\n",
        "plt.figure(figsize=(24,18))\n",
        "\n",
        "# heatmap\n",
        "sns.heatmap(cor, cmap=\"YlGnBu\", annot=True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgZA11LugHcn"
      },
      "source": [
        "Here we will observe the distribution of our classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IC5amYyZgHcp"
      },
      "outputs": [],
      "source": [
        "classes=df_credit['Class'].value_counts()\n",
        "normal_share=classes[0]/df_credit['Class'].count()*100\n",
        "fraud_share=classes[1]/df_credit['Class'].count()*100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        },
        "id": "jpode2vygHcu",
        "outputId": "af7ad557-f775-4b3d-d233-38218ab247e0"
      },
      "outputs": [],
      "source": [
        "# Create a bar plot for the number and percentage of fraudulent vs non-fraudulent transcations\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(12, 6))  # Adjusted for better visualization\n",
        "sns.countplot(x='Class', data=df_credit, palette=\"coolwarm\")\n",
        "\n",
        "# Formatting\n",
        "plt.title(\"Class Count\", fontsize=18)\n",
        "plt.xlabel(\"Record counts by class\", fontsize=14)\n",
        "plt.ylabel(\"Count\", fontsize=14)\n",
        "plt.xticks(fontsize=12)\n",
        "plt.yticks(fontsize=12)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        },
        "id": "1U9QPz1Pe3S3",
        "outputId": "8fc8ac9a-4700-4b2c-e9ea-7c6386fd21d8"
      },
      "outputs": [],
      "source": [
        "#plt.title('Dsitribution of the Fraudalent vs Non-fraudalent transaction in Percentages')\n",
        "classes=df_credit['Class'].value_counts()\n",
        "normal_share=classes[0]/df_credit['Class'].count()*100\n",
        "fraud_share=classes[1]/df_credit['Class'].count()*100\n",
        "\n",
        "labels = 'Non-Fraudalent', 'Fraudalent'\n",
        "sizes = [normal_share, fraud_share]\n",
        "explode = (0, 0.1)\n",
        "\n",
        "fig1, ax1 = plt.subplots()\n",
        "ax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',\n",
        "        shadow=True, startangle=90)\n",
        "ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "id": "YhXjoKgRe9GR",
        "outputId": "caca5be1-688e-49c3-ee8c-49546840b75c"
      },
      "outputs": [],
      "source": [
        "print('The percentage without churn prediction is ', round(df_credit['Class'].value_counts()[0]/len(df_credit) * 100,2), '% of the dataset')\n",
        "print('The percentage with churn prediction is ', round(df_credit['Class'].value_counts()[1]/len(df_credit) * 100,2), '% of the dataset')\n",
        "print('The ratio of imbalance is', round(df_credit['Class'].value_counts()[1]/df_credit['Class'].value_counts()[0] * 100,2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObdykDD1fA0i"
      },
      "source": [
        "### So we have 492 fraudalent transactions out of 284807 total credit card transactions.\n",
        "\n",
        "Target variable distribution shows that we are dealing with an highly imbalanced problem as there are many more genuine transactions class as compared to the fraudalent transactions. The model would achieve high accuracy as it would mostly predict majority class — transactions which are genuine in our example.\n",
        "To overcome this we will use other metrics for model evaluation such as ROC-AUC , precision and recall etc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EC08I57rgHc4"
      },
      "outputs": [],
      "source": [
        "# Create a scatter plot to observe the distribution of classes with time\n",
        "#As time is given in relative fashion, we will need to use pandas.Timedelta which Represents a duration, the difference between two dates or times.\n",
        "\n",
        "Delta_Time = pd.to_timedelta(df_credit['Time'], unit='s')\n",
        "#Create derived columns Mins and hours\n",
        "df_credit['Time_Day'] = (Delta_Time.dt.components.days).astype(int)\n",
        "df_credit['Time_Hour'] = (Delta_Time.dt.components.hours).astype(int)\n",
        "df_credit['Time_Min'] = (Delta_Time.dt.components.minutes).astype(int)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "M_Q5qrsVfIUa",
        "outputId": "b43b2d1b-e5be-4d21-e18c-d19f0910d1cf"
      },
      "outputs": [],
      "source": [
        "# Bivariate Analysis: Create a scatter plot to observe the distribution of classes with time\n",
        "\n",
        "fig = plt.figure(figsize=(14, 18))\n",
        "cmap = sns.color_palette('Set2')\n",
        "\n",
        "# PLot the relation between the variables:\n",
        "\n",
        "plt.subplot(3,1,1)\n",
        "sns.scatterplot(x=df_credit['Time'], y='Class', palette=cmap, data=df_credit)\n",
        "plt.xlabel('Time', size=18)\n",
        "plt.ylabel('Class', size=18)\n",
        "plt.tick_params(axis='x', labelsize=16)\n",
        "plt.tick_params(axis='y', labelsize=16)\n",
        "plt.title('Time vs Class Distribution', size=20, y=1.05)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "FdijLtf9WScD",
        "outputId": "72c1485e-61ca-49fd-8688-f96ee343cf5e"
      },
      "outputs": [],
      "source": [
        "#The fraus Vs normal trasaction by day\n",
        "plt.figure(figsize=(5,5))\n",
        "sns.distplot(df_credit[df_credit['Class'] == 0][\"Time_Day\"], color='green')\n",
        "sns.distplot(df_credit[df_credit['Class'] == 1][\"Time_Day\"], color='red')\n",
        "plt.title('Fraud Vs Normal Transactions by Day', fontsize=17)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "bIVgRFY_Wni0",
        "outputId": "804f611a-49df-4b45-eb21-dce69af879e8"
      },
      "outputs": [],
      "source": [
        "#The fraus Vs normal trasaction by hour\n",
        "plt.figure(figsize=(15,5))\n",
        "sns.distplot(df_credit[df_credit['Class'] == 0][\"Time_Hour\"], color='green')\n",
        "sns.distplot(df_credit[df_credit['Class'] == 1][\"Time_Hour\"], color='red')\n",
        "plt.title('Fraud Vs Normal Transactions by Hour', fontsize=17)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "id": "Y7jOKKkcfNDX",
        "outputId": "f2869da5-4dbc-41b0-e4a5-43a417c74eeb"
      },
      "outputs": [],
      "source": [
        "f, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(12,4))\n",
        "\n",
        "bins = 50\n",
        "\n",
        "ax1.hist(df_credit.Time[df_credit.Class == 1], bins = bins)\n",
        "ax1.set_title('Fraud')\n",
        "\n",
        "ax2.hist(df_credit.Time[df_credit.Class == 0], bins = bins)\n",
        "ax2.set_title('Normal')\n",
        "\n",
        "plt.xlabel('Time (in Seconds)')\n",
        "plt.ylabel('Number of Transactions')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "id": "U5n9CzwFgHc8",
        "outputId": "ee715e9a-0a5c-4de2-b1bf-aefc4eff51a5"
      },
      "outputs": [],
      "source": [
        "# Create a scatter plot to observe the distribution of classes with Amount\n",
        "#To clearly the data of frauds and no frauds\n",
        "df_Fraud = df_credit[df_credit['Class'] == 1]\n",
        "df_Regular = df_credit[df_credit['Class'] == 0]\n",
        "\n",
        "# Fraud Transaction Amount Statistics\n",
        "print(df_Fraud[\"Amount\"].describe())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "id": "zuzGv8BREZw3",
        "outputId": "a0839fbc-93be-4ca8-f1db-bb0836ef1017"
      },
      "outputs": [],
      "source": [
        "#Regular Transaction Amount Statistics\n",
        "print(df_Regular[\"Amount\"].describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "LeI5r5JDfRG-",
        "outputId": "a25b3ab5-16b9-4dc0-fa62-47ee34c91944"
      },
      "outputs": [],
      "source": [
        "# Create a scatter plot to observe the distribution of classes with Amount\n",
        "\n",
        "# Bivariate Analysis: Create a scatter plot to observe the distribution of classes with Amount\n",
        "\n",
        "\n",
        "fig = plt.figure(figsize=(14, 18))\n",
        "cmap = sns.color_palette('Set1')\n",
        "\n",
        "# PLot the relation between the variables:\n",
        "\n",
        "plt.subplot(3,1,1)\n",
        "sns.scatterplot(x=df_credit['Amount'], y='Class', palette=cmap, data=df_credit)\n",
        "plt.xlabel('Amount', size=18)\n",
        "plt.ylabel('Class', size=18)\n",
        "plt.tick_params(axis='x', labelsize=16)\n",
        "plt.tick_params(axis='y', labelsize=16)\n",
        "plt.title('Amount vs Class Distribution', size=20, y=1.05)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "id": "HyjiWmuZfT3j",
        "outputId": "283a36ce-470a-42dd-f680-3f638f9ea281"
      },
      "outputs": [],
      "source": [
        "f, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(12,4))\n",
        "\n",
        "bins = 30\n",
        "\n",
        "ax1.hist(df_credit.Amount[df_credit.Class == 1], bins = bins)\n",
        "ax1.set_title('Fraud')\n",
        "\n",
        "ax2.hist(df_credit.Amount[df_credit.Class == 0], bins = bins)\n",
        "ax2.set_title('Normal')\n",
        "\n",
        "plt.xlabel('Amount ($)')\n",
        "plt.ylabel('Number of Transactions')\n",
        "plt.yscale('log')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "dF_KtQq_fW6d",
        "outputId": "c1a17613-ab29-44d1-9cc8-cf10d726bdfe"
      },
      "outputs": [],
      "source": [
        "# Understanding more on the correlation in data:\n",
        "print(\"Most important features relative to target variable Class\")\n",
        "\n",
        "corr_initial = df_credit.corr()['Class']\n",
        "# convert series to dataframe so it can be sorted\n",
        "corr_initial = pd.DataFrame(corr_initial)\n",
        "# correct column label from SalePrice to correlation\n",
        "corr_initial.columns = [\"Correlation\"]\n",
        "# sort correlation\n",
        "corr_initial2 = corr_initial.sort_values(by=['Correlation'], ascending=False)\n",
        "corr_initial2.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "id": "oW3Rxih1fZv2",
        "outputId": "a665d066-c76f-4f60-9f85-4b3da9fd8513"
      },
      "outputs": [],
      "source": [
        "# Lets plot the heatmap again for relatively strong correlation (i.e. >0.09) with the target variable:\n",
        "\n",
        "top_feature = cor.index[abs(cor['Class']>0.09)]\n",
        "plt.subplots(figsize=(8, 5))\n",
        "top_corr = df_credit[top_feature].corr()\n",
        "sns.heatmap(top_corr, annot=True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuhHKcQOfjBW"
      },
      "source": [
        "### Plotting the distribution of a variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "p466zximfgYK",
        "outputId": "6141ae98-920a-4d7a-8636-dec3456c210c"
      },
      "outputs": [],
      "source": [
        "# Boxplot to understand the distribution of numerical attributes :\n",
        "\n",
        "# Selecting only numerical feature from the dataframe:\n",
        "numeric_features = df_credit.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "# Excluding BINARY target feature and Time variable as its not needed for transformation :\n",
        "li_not_plot = ['Class','Time']\n",
        "li_transform_num_feats = [c for c in list(numeric_features) if c not in li_not_plot]\n",
        "\n",
        "sns.set_style(\"whitegrid\")\n",
        "f, ax = plt.subplots(figsize=(22,34))\n",
        "# Using log scale:\n",
        "#ax.set_xscale(\"log\")\n",
        "ax = sns.boxplot(data=df_credit[li_transform_num_feats] , orient=\"h\", palette=\"Paired\")\n",
        "ax.set(ylabel=\"Features\")\n",
        "ax.set(xlabel=\"Values\")\n",
        "ax.set(title=\"Distribution of numerical atributes\")\n",
        "sns.despine(trim=True,left=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A5j6X1yfgHdB"
      },
      "outputs": [],
      "source": [
        "# Drop unnecessary columns\n",
        "# As we have derived the Day/Hour/Minutes from the time column we will drop Time\n",
        "df_credit.drop('Time', axis = 1, inplace= True)\n",
        "#also day/minutes might not be very useful as this is not time series data, we will keep only derived column hour\n",
        "df_credit.drop(['Time_Day', 'Time_Min'], axis = 1, inplace= True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "id": "cdryo91-frG9",
        "outputId": "20fe5f56-7c80-49ba-86c8-98f1de60dccb"
      },
      "outputs": [],
      "source": [
        "# Let's try to understand the Amount variable as it is not PCA transformed variable :\n",
        "\n",
        "plt.figure(figsize=(24, 12))\n",
        "\n",
        "plt.subplot(2,2,1)\n",
        "plt.title('Amount Distribution')\n",
        "df_credit['Amount'].astype(int).plot.hist();\n",
        "plt.xlabel(\"variable Amount\")\n",
        "#plt.ylabel(\"Frequency\")\n",
        "\n",
        "plt.subplot(2,2,2)\n",
        "plt.title('Amount Distribution')\n",
        "sns.set()\n",
        "plt.xlabel(\"variable Amount\")\n",
        "plt.hist(df_credit['Amount'],bins=100)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNPjeD7JgHdG"
      },
      "source": [
        "### Splitting the data into train & test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1LFQWczxgHdI"
      },
      "outputs": [],
      "source": [
        "#Create X and y dataset for independent and dependent data\n",
        "y= df_credit['Class']\n",
        "X = df_credit.drop(['Class'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "TUM02NQ6cjdE",
        "outputId": "3d832fd7-7cd3-46cb-d631-b117b986ec9f"
      },
      "outputs": [],
      "source": [
        "X.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sH30cd1_gHdO"
      },
      "outputs": [],
      "source": [
        "from sklearn import model_selection\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=100, test_size=0.20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6wrUtYGgHdW"
      },
      "source": [
        "##### Preserve X_test & y_test to evaluate on the test data once you build the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "id": "N6XYz1PxgHdX",
        "outputId": "eaf2abd5-6b84-4aa6-8232-1f23f4e9e59b"
      },
      "outputs": [],
      "source": [
        "print(np.sum(y))\n",
        "print(np.sum(y_train))\n",
        "print(np.sum(y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2FsTuiUgHdh"
      },
      "source": [
        "### Plotting the distribution of a variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zlh16WqYlEt2"
      },
      "outputs": [],
      "source": [
        "cols = list(X.columns.values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "id": "jEaTjnymmmEH",
        "outputId": "d46cf2fd-73d7-481f-8b5d-e35e778eea1e"
      },
      "outputs": [],
      "source": [
        "cols"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DnpcWhFogHdj",
        "outputId": "d574e659-b7b0-4ec2-d31e-6bbd8d4fce70"
      },
      "outputs": [],
      "source": [
        "# plot the histogram of a variable from the dataset to see the skewness\n",
        "normal_records = df_credit.Class == 0\n",
        "fraud_records = df_credit.Class == 1\n",
        "\n",
        "plt.figure(figsize=(20, 60))\n",
        "for n, col in enumerate(cols):\n",
        "  plt.subplot(10,3,n+1)\n",
        "  sns.distplot(X[col][normal_records], color='green')\n",
        "  sns.distplot(X[col][fraud_records], color='red')\n",
        "  plt.title(col, fontsize=17)\n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZUF74Zvlawv"
      },
      "source": [
        "#Create model functions for Logistic Regress, KNN, SVM, Decision Tree, Random Forest, XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4bpQNp8ZuCW"
      },
      "outputs": [],
      "source": [
        "#Create a dataframe to store results\n",
        "df_Results = pd.DataFrame(columns=['Data_Imbalance_Handiling','Model','Accuracy','roc_value','threshold'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9_GGCOMhkKf"
      },
      "outputs": [],
      "source": [
        "def Plot_confusion_matrix(y_test, pred_test):\n",
        "  cm = confusion_matrix(y_test, pred_test)\n",
        "  plt.clf()\n",
        "  plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Accent)\n",
        "  classNames = ['Non-Fraudalent','Fraudalent']\n",
        "  plt.title('Confusion Matrix - Test Data')\n",
        "  plt.ylabel('True label')\n",
        "  plt.xlabel('Predicted label')\n",
        "  tick_marks = np.arange(len(classNames))\n",
        "  plt.xticks(tick_marks, classNames, rotation=45)\n",
        "  plt.yticks(tick_marks, classNames)\n",
        "  s = [['TN','FP'], ['FN', 'TP']]\n",
        "\n",
        "  for i in range(2):\n",
        "      for j in range(2):\n",
        "          plt.text(j,i, str(s[i][j])+\" = \"+str(cm[i][j]),fontsize=12)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OTR8jrpqa3eW"
      },
      "outputs": [],
      "source": [
        "def buildAndRunLogisticModels(df_Results, DataImabalance, X_train,y_train, X_test, y_test ):\n",
        "\n",
        "  # Logistic Regression\n",
        "  from sklearn import linear_model #import the package\n",
        "  from sklearn.model_selection import KFold\n",
        "\n",
        "  num_C = list(np.power(10.0, np.arange(-10, 10)))\n",
        "  cv_num = KFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "  searchCV_l2 = linear_model.LogisticRegressionCV(\n",
        "          Cs= num_C\n",
        "          ,penalty='l2'\n",
        "          ,scoring='roc_auc'\n",
        "          ,cv=cv_num\n",
        "          ,random_state=42\n",
        "          ,max_iter=10000\n",
        "          ,fit_intercept=True\n",
        "          ,solver='newton-cg'\n",
        "          ,tol=10\n",
        "      )\n",
        "\n",
        "  searchCV_l1 = linear_model.LogisticRegressionCV(\n",
        "          Cs=num_C\n",
        "          ,penalty='l1'\n",
        "          ,scoring='roc_auc'\n",
        "          ,cv=cv_num\n",
        "          ,random_state=42\n",
        "          ,max_iter=10000\n",
        "          ,fit_intercept=True\n",
        "          ,solver='liblinear'\n",
        "          ,tol=10\n",
        "      )\n",
        "  #searchCV.fit(X_train, y_train)\n",
        "  searchCV_l2.fit(X_train, y_train)\n",
        "  searchCV_l1.fit(X_train, y_train)\n",
        "  print ('Max auc_roc for l2:', searchCV_l2.scores_[1].mean(axis=0).max())\n",
        "  print ('Max auc_roc for l1:', searchCV_l1.scores_[1].mean(axis=0).max())\n",
        "\n",
        "  print(\"Parameters for l2 regularisations\")\n",
        "  print(searchCV_l2.coef_)\n",
        "  print(searchCV_l2.intercept_)\n",
        "  print(searchCV_l2.scores_)\n",
        "\n",
        "  print(\"Parameters for l1 regularisations\")\n",
        "  print(searchCV_l1.coef_)\n",
        "  print(searchCV_l1.intercept_)\n",
        "  print(searchCV_l1.scores_)\n",
        "\n",
        "\n",
        "  #find predicted vallues\n",
        "  y_pred_l2 = searchCV_l2.predict(X_test)\n",
        "  y_pred_l1 = searchCV_l1.predict(X_test)\n",
        "\n",
        "\n",
        "  #Find predicted probabilities\n",
        "  y_pred_probs_l2 = searchCV_l2.predict_proba(X_test)[:,1]\n",
        "  y_pred_probs_l1 = searchCV_l1.predict_proba(X_test)[:,1]\n",
        "\n",
        "  # Accuaracy of L2/L1 models\n",
        "  Accuracy_l2 = metrics.accuracy_score(y_pred=y_pred_l2, y_true=y_test)\n",
        "  Accuracy_l1 = metrics.accuracy_score(y_pred=y_pred_l1, y_true=y_test)\n",
        "\n",
        "  print(\"Accuarcy of Logistic model with l2 regularisation : {0}\".format(Accuracy_l2))\n",
        "  print(\"Confusion Matrix\")\n",
        "  Plot_confusion_matrix(y_test, y_pred_l2)\n",
        "  print(\"classification Report\")\n",
        "  print(classification_report(y_test, y_pred_l2))\n",
        "  print(\"Accuarcy of Logistic model with l1 regularisation : {0}\".format(Accuracy_l1))\n",
        "  print(\"Confusion Matrix\")\n",
        "  Plot_confusion_matrix(y_test, y_pred_l1)\n",
        "  print(\"classification Report\")\n",
        "  print(classification_report(y_test, y_pred_l1))\n",
        "\n",
        "  from sklearn.metrics import roc_auc_score\n",
        "  l2_roc_value = roc_auc_score(y_test, y_pred_probs_l2)\n",
        "  print(\"l2 roc_value: {0}\" .format(l2_roc_value))\n",
        "  fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred_probs_l2)\n",
        "  threshold = thresholds[np.argmax(tpr-fpr)]\n",
        "  print(\"l2 threshold: {0}\".format(threshold))\n",
        "\n",
        "  roc_auc = metrics.auc(fpr, tpr)\n",
        "  print(\"ROC for the test dataset\",'{:.1%}'.format(roc_auc))\n",
        "  plt.plot(fpr,tpr,label=\"Test, auc=\"+str(roc_auc))\n",
        "  plt.legend(loc=4)\n",
        "  plt.show()\n",
        "\n",
        "  df_Results = pd.concat([df_Results, \n",
        "                        pd.DataFrame({'Data_Imbalance_Handiling': [DataImabalance],\n",
        "                                      'Model': ['Logistic Regression with L2 Regularisation'],\n",
        "                                      'Accuracy': [Accuracy_l2],\n",
        "                                      'roc_value': [l2_roc_value],\n",
        "                                      'threshold': [threshold]})], \n",
        "                        ignore_index=True)\n",
        "\n",
        "  l1_roc_value = roc_auc_score(y_test, y_pred_probs_l1)\n",
        "  print(\"l1 roc_value: {0}\" .format(l1_roc_value))\n",
        "  fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred_probs_l1)\n",
        "  threshold = thresholds[np.argmax(tpr-fpr)]\n",
        "  print(\"l1 threshold: {0}\".format(threshold))\n",
        "\n",
        "  roc_auc = metrics.auc(fpr, tpr)\n",
        "  print(\"ROC for the test dataset\",'{:.1%}'.format(roc_auc))\n",
        "  plt.plot(fpr,tpr,label=\"Test, auc=\"+str(roc_auc))\n",
        "  plt.legend(loc=4)\n",
        "  plt.show()\n",
        "  \n",
        "  df_Results = pd.concat([df_Results, \n",
        "                        pd.DataFrame({'Data_Imbalance_Handiling': [DataImabalance],\n",
        "                                      'Model': ['Logistic Regression with L1 Regularisation'],\n",
        "                                      'Accuracy': [Accuracy_l1],\n",
        "                                      'roc_value': [l1_roc_value],\n",
        "                                      'threshold': [threshold]})], \n",
        "                        ignore_index=True)\n",
        "\n",
        "  return df_Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3QyvGb7JG1JV"
      },
      "outputs": [],
      "source": [
        "def buildAndRunKNNModels(df_Results,DataImabalance, X_train,y_train, X_test, y_test ):\n",
        "  #Evaluate KNN  model\n",
        "  from sklearn.neighbors import KNeighborsClassifier\n",
        "  from sklearn.metrics import roc_auc_score\n",
        "  #create KNN model and fit the model with train dataset\n",
        "  knn = KNeighborsClassifier(n_neighbors = 5,n_jobs=16)\n",
        "  knn.fit(X_train,y_train)\n",
        "  score = knn.score(X_test,y_test)\n",
        "  print(\"model score\")\n",
        "  print(score)\n",
        "\n",
        "  #Accuracy\n",
        "  y_pred = knn.predict(X_test)\n",
        "  KNN_Accuracy = metrics.accuracy_score(y_pred=y_pred, y_true=y_test)\n",
        "  print(\"Confusion Matrix\")\n",
        "  Plot_confusion_matrix(y_test, y_pred)\n",
        "  print(\"classification Report\")\n",
        "  print(classification_report(y_test, y_pred))\n",
        "\n",
        "\n",
        "  knn_probs = knn.predict_proba(X_test)[:, 1]\n",
        "\n",
        "  # Calculate roc auc\n",
        "  knn_roc_value = roc_auc_score(y_test, knn_probs)\n",
        "  print(\"KNN roc_value: {0}\" .format(knn_roc_value))\n",
        "  fpr, tpr, thresholds = metrics.roc_curve(y_test, knn_probs)\n",
        "  threshold = thresholds[np.argmax(tpr-fpr)]\n",
        "  print(\"KNN threshold: {0}\".format(threshold))\n",
        "\n",
        "  roc_auc = metrics.auc(fpr, tpr)\n",
        "  print(\"ROC for the test dataset\",'{:.1%}'.format(roc_auc))\n",
        "  plt.plot(fpr,tpr,label=\"Test, auc=\"+str(roc_auc))\n",
        "  plt.legend(loc=4)\n",
        "  plt.show()\n",
        "\n",
        "  df_Results = pd.concat([df_Results, \n",
        "                        pd.DataFrame({'Data_Imbalance_Handiling': [DataImabalance],\n",
        "                                      'Model': ['KNN'],\n",
        "                                      'Accuracy': [score],\n",
        "                                      'roc_value': [knn_roc_value],\n",
        "                                      'threshold': [threshold]})], \n",
        "                        ignore_index=True)\n",
        "\n",
        "  return df_Results\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTwKJajpIBrv"
      },
      "outputs": [],
      "source": [
        "def buildAndRunSVMModels(df_Results, DataImabalance, X_train,y_train, X_test, y_test ):\n",
        "  #Evaluate SVM model with sigmoid kernel  model\n",
        "  from sklearn.svm import SVC\n",
        "  from sklearn.metrics import accuracy_score\n",
        "  from sklearn.metrics import roc_auc_score\n",
        "\n",
        "  clf = SVC(kernel='sigmoid', random_state=42)\n",
        "  clf.fit(X_train,y_train)\n",
        "  y_pred_SVM = clf.predict(X_test)\n",
        "  SVM_Score = accuracy_score(y_test,y_pred_SVM)\n",
        "  print(\"accuracy_score : {0}\".format(SVM_Score))\n",
        "  print(\"Confusion Matrix\")\n",
        "  Plot_confusion_matrix(y_test, y_pred_SVM)\n",
        "  print(\"classification Report\")\n",
        "  print(classification_report(y_test, y_pred_SVM))\n",
        "\n",
        "\n",
        "\n",
        "  # Run classifier\n",
        "  classifier = SVC(kernel='sigmoid' , probability=True)\n",
        "  svm_probs = classifier.fit(X_train, y_train).predict_proba(X_test)[:, 1]\n",
        "\n",
        "  # Calculate roc auc\n",
        "  roc_value = roc_auc_score(y_test, svm_probs)\n",
        "\n",
        "  print(\"SVM roc_value: {0}\" .format(roc_value))\n",
        "  fpr, tpr, thresholds = metrics.roc_curve(y_test, svm_probs)\n",
        "  threshold = thresholds[np.argmax(tpr-fpr)]\n",
        "  print(\"SVM threshold: {0}\".format(threshold))\n",
        "  roc_auc = metrics.auc(fpr, tpr)\n",
        "  print(\"ROC for the test dataset\",'{:.1%}'.format(roc_auc))\n",
        "  plt.plot(fpr,tpr,label=\"Test, auc=\"+str(roc_auc))\n",
        "  plt.legend(loc=4)\n",
        "  plt.show()\n",
        "\n",
        "  df_Results = pd.concat([df_Results, \n",
        "                        pd.DataFrame({'Data_Imbalance_Handiling': [DataImabalance],\n",
        "                                      'Model': ['SVM'],\n",
        "                                      'Accuracy': [SVM_Score],\n",
        "                                      'roc_value': [roc_value],\n",
        "                                      'threshold': [threshold]})], \n",
        "                        ignore_index=True)\n",
        "  return df_Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xbYxW-ZYIlJ9"
      },
      "outputs": [],
      "source": [
        "def buildAndRunTreeModels(df_Results, DataImabalance, X_train,y_train, X_test, y_test ):\n",
        "  #Evaluate Decision Tree model with 'gini' & 'entropy'\n",
        "  from sklearn.tree import DecisionTreeClassifier\n",
        "  from sklearn.metrics import roc_auc_score\n",
        "  criteria = ['gini', 'entropy']\n",
        "  scores = {}\n",
        "\n",
        "  for c in criteria:\n",
        "      dt = DecisionTreeClassifier(criterion = c, random_state=42)\n",
        "      dt.fit(X_train, y_train)\n",
        "      y_pred = dt.predict(X_test)\n",
        "      test_score = dt.score(X_test, y_test)\n",
        "      tree_preds = dt.predict_proba(X_test)[:, 1]\n",
        "      tree_roc_value = roc_auc_score(y_test, tree_preds)\n",
        "      scores = test_score\n",
        "      print(c + \" score: {0}\" .format(test_score))\n",
        "      print(\"Confusion Matrix\")\n",
        "      Plot_confusion_matrix(y_test, y_pred)\n",
        "      print(\"classification Report\")\n",
        "      print(classification_report(y_test, y_pred))\n",
        "      print(c + \" tree_roc_value: {0}\" .format(tree_roc_value))\n",
        "      fpr, tpr, thresholds = metrics.roc_curve(y_test, tree_preds)\n",
        "      threshold = thresholds[np.argmax(tpr-fpr)]\n",
        "      print(\"Tree threshold: {0}\".format(threshold))\n",
        "      roc_auc = metrics.auc(fpr, tpr)\n",
        "      print(\"ROC for the test dataset\",'{:.1%}'.format(roc_auc))\n",
        "      plt.plot(fpr,tpr,label=\"Test, auc=\"+str(roc_auc))\n",
        "      plt.legend(loc=4)\n",
        "      plt.show()\n",
        "\n",
        "      df_Results = pd.concat([df_Results, \n",
        "                        pd.DataFrame({'Data_Imbalance_Handiling': [DataImabalance],\n",
        "                                      'Model': [f'Tree Model with {c} criteria'],\n",
        "                                      'Accuracy': [test_score],\n",
        "                                      'roc_value': [tree_roc_value],\n",
        "                                      'threshold': [threshold]})], \n",
        "                        ignore_index=True)\n",
        "\n",
        "  return df_Results\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTSuL30IJAol"
      },
      "outputs": [],
      "source": [
        "def buildAndRunRandomForestModels(df_Results, DataImabalance, X_train,y_train, X_test, y_test ):\n",
        "  #Evaluate Random Forest model\n",
        "\n",
        "  from sklearn.ensemble import RandomForestClassifier\n",
        "  from sklearn.metrics import roc_auc_score\n",
        "\n",
        "  # Create the model with 100 trees\n",
        "  RF_model = RandomForestClassifier(n_estimators=100,\n",
        "                                bootstrap = True,\n",
        "                                max_features = 'sqrt', random_state=42)\n",
        "  # Fit on training data\n",
        "  RF_model.fit(X_train, y_train)\n",
        "  RF_test_score = RF_model.score(X_test, y_test)\n",
        "  RF_model.predict(X_test)\n",
        "\n",
        "  print('Model Accuracy: {0}'.format(RF_test_score))\n",
        "\n",
        "\n",
        "  # Actual class predictions\n",
        "  rf_predictions = RF_model.predict(X_test)\n",
        "\n",
        "  print(\"Confusion Matrix\")\n",
        "  Plot_confusion_matrix(y_test, rf_predictions)\n",
        "  print(\"classification Report\")\n",
        "  print(classification_report(y_test, rf_predictions))\n",
        "\n",
        "  # Probabilities for each class\n",
        "  rf_probs = RF_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "  # Calculate roc auc\n",
        "  roc_value = roc_auc_score(y_test, rf_probs)\n",
        "\n",
        "  print(\"Random Forest roc_value: {0}\" .format(roc_value))\n",
        "  fpr, tpr, thresholds = metrics.roc_curve(y_test, rf_probs)\n",
        "  threshold = thresholds[np.argmax(tpr-fpr)]\n",
        "  print(\"Random Forest threshold: {0}\".format(threshold))\n",
        "  roc_auc = metrics.auc(fpr, tpr)\n",
        "  print(\"ROC for the test dataset\",'{:.1%}'.format(roc_auc))\n",
        "  plt.plot(fpr,tpr,label=\"Test, auc=\"+str(roc_auc))\n",
        "  plt.legend(loc=4)\n",
        "  plt.show()\n",
        "\n",
        "  df_Results = pd.concat([df_Results, \n",
        "                        pd.DataFrame({'Data_Imbalance_Handiling': [DataImabalance],\n",
        "                                      'Model': ['Random Forest'],\n",
        "                                      'Accuracy': [RF_test_score],\n",
        "                                      'roc_value': [roc_value],\n",
        "                                      'threshold': [threshold]})], \n",
        "                        ignore_index=True)\n",
        "  return df_Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R8j7DBKaJN0Z"
      },
      "outputs": [],
      "source": [
        "def buildAndRunXGBoostModels(df_Results, DataImabalance,X_train,y_train, X_test, y_test ):\n",
        "  #Evaluate XGboost model\n",
        "  from xgboost import XGBClassifier\n",
        "  from sklearn.metrics import roc_auc_score\n",
        "  # fit model no training data\n",
        "  XGBmodel = XGBClassifier(random_state=42)\n",
        "  XGBmodel.fit(X_train, y_train)\n",
        "  y_pred = XGBmodel.predict(X_test)\n",
        "\n",
        "  XGB_test_score = XGBmodel.score(X_test, y_test)\n",
        "  print('Model Accuracy: {0}'.format(XGB_test_score))\n",
        "\n",
        "  print(\"Confusion Matrix\")\n",
        "  Plot_confusion_matrix(y_test, y_pred)\n",
        "  print(\"classification Report\")\n",
        "  print(classification_report(y_test, y_pred))\n",
        "  # Probabilities for each class\n",
        "  XGB_probs = XGBmodel.predict_proba(X_test)[:, 1]\n",
        "\n",
        "  # Calculate roc auc\n",
        "  XGB_roc_value = roc_auc_score(y_test, XGB_probs)\n",
        "\n",
        "  print(\"XGboost roc_value: {0}\" .format(XGB_roc_value))\n",
        "  fpr, tpr, thresholds = metrics.roc_curve(y_test, XGB_probs)\n",
        "  threshold = thresholds[np.argmax(tpr-fpr)]\n",
        "  print(\"XGBoost threshold: {0}\".format(threshold))\n",
        "  roc_auc = metrics.auc(fpr, tpr)\n",
        "  print(\"ROC for the test dataset\",'{:.1%}'.format(roc_auc))\n",
        "  plt.plot(fpr,tpr,label=\"Test, auc=\"+str(roc_auc))\n",
        "  plt.legend(loc=4)\n",
        "  plt.show()\n",
        "\n",
        "  df_Results = pd.concat([df_Results, \n",
        "                        pd.DataFrame({'Data_Imbalance_Handiling': [DataImabalance],\n",
        "                                      'Model': ['XGBoost'],\n",
        "                                      'Accuracy': [XGB_test_score],\n",
        "                                      'roc_value': [XGB_roc_value],\n",
        "                                      'threshold': [threshold]})], \n",
        "                        ignore_index=True)\n",
        "  return df_Results\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgrN2lUcgHdo"
      },
      "source": [
        "### If there is skewness present in the distribution use:\n",
        "- <b>Power Transformer</b> package present in the <b>preprocessing library provided by sklearn</b> to make distribution more gaussian"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qRQcO1mogHdr"
      },
      "outputs": [],
      "source": [
        "# - Apply : preprocessing.PowerTransformer(copy=False) to fit & transform the train & test data\n",
        "from sklearn.preprocessing import PolynomialFeatures, PowerTransformer\n",
        "pt = PowerTransformer()\n",
        "pt.fit(X_train)                       ## Fit the PT on training data\n",
        "X_train_pt = pt.transform(X_train)    ## Then apply on all data\n",
        "X_test_pt = pt.transform(X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IpuMuYcDuJvi"
      },
      "outputs": [],
      "source": [
        "#Create Dataframe\n",
        "X_train_pt_df = pd.DataFrame(data=X_train_pt,   columns=cols)\n",
        "X_test_pt_df = pd.DataFrame(data=X_test_pt,   columns=cols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0sb7XBTxgHd0",
        "outputId": "0a9c9fbc-9fcc-4d73-ccda-7cc0121e0ca3"
      },
      "outputs": [],
      "source": [
        "# plot the histogram of a variable from the train dataset again to see the result\n",
        "\n",
        "plt.figure(figsize=(20, 60))\n",
        "for n, col in enumerate(cols):\n",
        "  plt.subplot(10,3,n+1)\n",
        "  sns.distplot(X_train_pt_df[col][normal_records], color='green')\n",
        "  sns.distplot(X_train_pt_df[col][fraud_records], color='red')\n",
        "  plt.title(col, fontsize=17)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "J2G2RUgdxeJm",
        "outputId": "dbac2bd3-5ffb-4c87-b51b-929b76e8fba8"
      },
      "outputs": [],
      "source": [
        "# plot the histogram of a variable from the test dataset again to see the result\n",
        "plt.figure(figsize=(20, 60))\n",
        "for n, col in enumerate(cols):\n",
        "  plt.subplot(10,3,n+1)\n",
        "  sns.distplot(X_test_pt_df[col][normal_records], color='green')\n",
        "  sns.distplot(X_test_pt_df[col][fraud_records], color='red')\n",
        "  plt.title(col, fontsize=17)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7xlqWAogHd4"
      },
      "source": [
        "## Model Building\n",
        "- Build different models on the imbalanced dataset and see the result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6VqZZBarm__S",
        "outputId": "9c6a30db-a2c3-4849-a7ff-7cb087a11f0d"
      },
      "outputs": [],
      "source": [
        "#Run Logistic Regression with L1 And L2 Regularisation\n",
        "print(\"Logistic Regression with L1 And L2 Regularisation\")\n",
        "start_time = time.time()\n",
        "df_Results = buildAndRunLogisticModels(df_Results,\"Power Transformer\",X_train_pt_df,y_train, X_test_pt_df, y_test)\n",
        "print(\"Time Taken by Model: --- %s seconds ---\" % (time.time() - start_time))\n",
        "print('-'*80 )\n",
        "#Run KNN Model\n",
        "print(\"KNN Model\")\n",
        "start_time = time.time()\n",
        "df_Results = buildAndRunKNNModels(df_Results,\"Power Transformer\",X_train_pt_df,y_train, X_test_pt_df, y_test)\n",
        "print(\"Time Taken by Model: --- %s seconds ---\" % (time.time() - start_time))\n",
        "print('-'*80 )\n",
        "#Run Decision Tree Models with  'gini' & 'entropy' criteria\n",
        "print(\"Decision Tree Models with  'gini' & 'entropy' criteria\")\n",
        "start_time = time.time()\n",
        "df_Results = buildAndRunTreeModels(df_Results,\"Power Transformer\",X_train_pt_df,y_train, X_test_pt_df, y_test)\n",
        "print(\"Time Taken by Model: --- %s seconds ---\" % (time.time() - start_time))\n",
        "print('-'*80 )\n",
        "#Run Random Forest Model\n",
        "print(\"Random Forest Model\")\n",
        "start_time = time.time()\n",
        "df_Results = buildAndRunRandomForestModels(df_Results,\"Power Transformer\",X_train_pt_df,y_train, X_test_pt_df, y_test)\n",
        "print(\"Time Taken by Model: --- %s seconds ---\" % (time.time() - start_time))\n",
        "print('-'*80 )\n",
        "#Run XGBoost Modela\n",
        "print(\"XGBoost Model\")\n",
        "start_time = time.time()\n",
        "df_Results = buildAndRunXGBoostModels(df_Results,\"Power Transformer\",X_train_pt_df,y_train, X_test_pt_df, y_test)\n",
        "print(\"Time Taken by Model: --- %s seconds ---\" % (time.time() - start_time))\n",
        "print('-'*80 )\n",
        "#Run SVM Model with Sigmoid Kernel\n",
        "print(\"SVM Model with Sigmoid Kernel\")\n",
        "start_time = time.time()\n",
        "df_Results = buildAndRunSVMModels(df_Results,\"Power Transformer\",X_train_pt_df,y_train, X_test_pt_df, y_test)\n",
        "print(\"Time Taken by Model: --- %s seconds ---\" % (time.time() - start_time))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "bpBvQio7qmya",
        "outputId": "93e73783-bb96-48b6-a774-8ad5cb67d839"
      },
      "outputs": [],
      "source": [
        "df_Results.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHrsS6hvPAGe"
      },
      "source": [
        "# Perform cross validation with RepeatedKFold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        },
        "id": "TI1-COB9pHkx",
        "outputId": "359492c2-7777-44ee-b802-f8131b54138a"
      },
      "outputs": [],
      "source": [
        "#Lets perfrom RepeatedKFold and check the results\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "rkf = RepeatedKFold(n_splits=5, n_repeats=10, random_state=None)\n",
        "# X is the feature set and y is the target\n",
        "for train_index, test_index in rkf.split(X,y):\n",
        "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
        "    X_train_cv, X_test_cv = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_train_cv, y_test_cv = y.iloc[train_index], y.iloc[test_index]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9i7w_vRsMRsD",
        "outputId": "13cc445f-0ef4-4150-f548-8a22d5a48157"
      },
      "outputs": [],
      "source": [
        "#Run Logistic Regression with L1 And L2 Regularisation\n",
        "print(\"Logistic Regression with L1 And L2 Regularisation\")\n",
        "start_time = time.time()\n",
        "df_Results = buildAndRunLogisticModels(df_Results,\"RepeatedKFold Cross Validation\", X_train_cv,y_train_cv, X_test_cv, y_test_cv)\n",
        "print(\"Time Taken by Model: --- %s seconds ---\" % (time.time() - start_time))\n",
        "print('-'*80 )\n",
        "#Run KNN Model\n",
        "print(\"KNN Model\")\n",
        "start_time = time.time()\n",
        "df_Results = buildAndRunKNNModels(df_Results,\"RepeatedKFold Cross Validation\",X_train_cv,y_train_cv, X_test_cv, y_test_cv)\n",
        "print(\"Time Taken by Model: --- %s seconds ---\" % (time.time() - start_time))\n",
        "print('-'*80 )\n",
        "#Run Decision Tree Models with  'gini' & 'entropy' criteria\n",
        "print(\"Decision Tree Models with  'gini' & 'entropy' criteria\")\n",
        "start_time = time.time()\n",
        "df_Results = buildAndRunTreeModels(df_Results,\"RepeatedKFold Cross Validation\",X_train_cv,y_train_cv, X_test_cv, y_test_cv)\n",
        "print(\"Time Taken by Model: --- %s seconds ---\" % (time.time() - start_time))\n",
        "print('-'*80 )\n",
        "#Run Random Forest Model\n",
        "print(\"Random Forest Model\")\n",
        "start_time = time.time()\n",
        "df_Results = buildAndRunRandomForestModels(df_Results,\"RepeatedKFold Cross Validation\",X_train_cv,y_train_cv, X_test_cv, y_test_cv)\n",
        "print(\"Time Taken by Model: --- %s seconds ---\" % (time.time() - start_time))\n",
        "print('-'*80 )\n",
        "#Run XGBoost Modela\n",
        "print(\"XGBoost Model\")\n",
        "start_time = time.time()\n",
        "df_Results = buildAndRunXGBoostModels(df_Results,\"RepeatedKFold Cross Validation\",X_train_cv,y_train_cv, X_test_cv, y_test_cv)\n",
        "print(\"Time Taken by Model: --- %s seconds ---\" % (time.time() - start_time))\n",
        "print('-'*80 )\n",
        "#Run SVM Model with Sigmoid Kernel\n",
        "print(\"SVM Model with Sigmoid Kernel\")\n",
        "start_time = time.time()\n",
        "df_Results = buildAndRunSVMModels(df_Results,\"RepeatedKFold Cross Validation\",X_train_cv,y_train_cv, X_test_cv, y_test_cv)\n",
        "print(\"Time Taken by Model: --- %s seconds ---\" % (time.time() - start_time))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "id": "Yk_Pnp_ZPqDe",
        "outputId": "abda9d87-d30b-475c-b5b0-c7c24640a569"
      },
      "outputs": [],
      "source": [
        "df_Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pILbBSWwr0LN"
      },
      "source": [
        "## It seems XGBOost with Repeated KFold cross validation has provided us wih best results with ROC_Value of 0.962596"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BdVRceJQsGik"
      },
      "outputs": [],
      "source": [
        "#Evaluate XGboost model\n",
        "from xgboost import XGBClassifier\n",
        "# fit model no training data\n",
        "XGBmodel = XGBClassifier(random_state=42)\n",
        "XGBmodel.fit(X_train_cv,y_train_cv)\n",
        "\n",
        "coefficients = pd.concat([pd.DataFrame(X.columns),pd.DataFrame(np.transpose(XGBmodel.feature_importances_))], axis = 1)\n",
        "coefficients.columns = ['Feature','feature_importances']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "id": "ieedbxrysFyO",
        "outputId": "f0bb4523-ee6c-4df8-9572-09e7c9331330"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20,5))\n",
        "sns.barplot(x='Feature', y='feature_importances', data=coefficients)\n",
        "plt.title(\"XGBoost with Repeated KFold Feature Importance\", fontsize=18)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Zx1jyw2PL4d"
      },
      "source": [
        "# Perform cross validation with StratifiedKFold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "id": "9oi7rmXoO7Gy",
        "outputId": "837cc0bb-d787-49b4-e6c6-d853f8b23ca4"
      },
      "outputs": [],
      "source": [
        "#Lets perfrom StratifiedKFold and check the results\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "skf = StratifiedKFold(n_splits=5, random_state=None)\n",
        "# X is the feature set and y is the target\n",
        "for train_index, test_index in skf.split(X,y):\n",
        "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
        "    X_train_SKF_cv, X_test_SKF_cv = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_train_SKF_cv, y_test_SKF_cv = y.iloc[train_index], y.iloc[test_index]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYCSJVoegHeH"
      },
      "source": [
        "### Similarly explore other algorithms by building models like:\n",
        "- KNN\n",
        "- SVM\n",
        "- Decision Tree\n",
        "- Random Forest\n",
        "- XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qW6Sbs-EO7WL",
        "outputId": "5e84fdd0-fb60-4fb4-c7aa-a1fa1df70aca"
      },
      "outputs": [],
      "source": [
        "#Run Logistic Regression with L1 And L2 Regularisation\n",
        "print(\"Logistic Regression with L1 And L2 Regularisation\")\n",
        "start_time = time.time()\n",
        "df_Results = buildAndRunLogisticModels(df_Results,\"StratifiedKFold Cross Validation\", X_train_SKF_cv,y_train_SKF_cv, X_test_SKF_cv, y_test_SKF_cv)\n",
        "print(\"Time Taken by Model: --- %s seconds ---\" % (time.time() - start_time))\n",
        "print('-'*80 )\n",
        "#Run KNN Model\n",
        "print(\"KNN Model\")\n",
        "start_time = time.time()\n",
        "df_Results = buildAndRunKNNModels(df_Results,\"StratifiedKFold Cross Validation\",X_train_SKF_cv,y_train_SKF_cv, X_test_SKF_cv, y_test_SKF_cv)\n",
        "print(\"Time Taken by Model: --- %s seconds ---\" % (time.time() - start_time))\n",
        "print('-'*80 )\n",
        "#Run Decision Tree Models with  'gini' & 'entropy' criteria\n",
        "print(\"Decision Tree Models with  'gini' & 'entropy' criteria\")\n",
        "start_time = time.time()\n",
        "df_Results = buildAndRunTreeModels(df_Results,\"StratifiedKFold Cross Validation\",X_train_SKF_cv,y_train_SKF_cv, X_test_SKF_cv, y_test_SKF_cv)\n",
        "print(\"Time Taken by Model: --- %s seconds ---\" % (time.time() - start_time))\n",
        "print('-'*80 )\n",
        "#Run Random Forest Model\n",
        "print(\"Random Forest Model\")\n",
        "start_time = time.time()\n",
        "df_Results = buildAndRunRandomForestModels(df_Results,\"StratifiedKFold Cross Validation\",X_train_SKF_cv,y_train_SKF_cv, X_test_SKF_cv, y_test_SKF_cv)\n",
        "print(\"Time Taken by Model: --- %s seconds ---\" % (time.time() - start_time))\n",
        "print('-'*80 )\n",
        "#Run XGBoost Modela\n",
        "print(\"XGBoost Model\")\n",
        "start_time = time.time()\n",
        "df_Results = buildAndRunXGBoostModels(df_Results,\"StratifiedKFold Cross Validation\",X_train_SKF_cv,y_train_SKF_cv, X_test_SKF_cv, y_test_SKF_cv)\n",
        "print(\"Time Taken by Model: --- %s seconds ---\" % (time.time() - start_time))\n",
        "print('-'*80 )\n",
        "#Run SVM Model with Sigmoid Kernel\n",
        "print(\"SVM Model with Sigmoid Kernel\")\n",
        "start_time = time.time()\n",
        "df_Results = buildAndRunSVMModels(df_Results,\"StratifiedKFold Cross Validation\",X_train_SKF_cv,y_train_SKF_cv, X_test_SKF_cv, y_test_SKF_cv)\n",
        "print(\"Time Taken by Model: --- %s seconds ---\" % (time.time() - start_time))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 793
        },
        "id": "lqusMH_a3G5Y",
        "outputId": "48df4394-97d5-44e4-84a8-fcf28915851c"
      },
      "outputs": [],
      "source": [
        "df_Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9EgNnAK_CTs"
      },
      "source": [
        "**As the results show Logistic Regression with L2 Regularisation for StratifiedFold cross validation provided best results**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXLqBKrwgHeI"
      },
      "source": [
        "#### Proceed with the model which shows the best result\n",
        "- Apply the best hyperparameter on the model\n",
        "- Predict on the test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 935
        },
        "id": "YBJV34yHKqVY",
        "outputId": "b88a1c11-aa41-4aa2-b6ce-46d9cdd57618"
      },
      "outputs": [],
      "source": [
        "  # Logistic Regression\n",
        "from sklearn import linear_model #import the package\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "num_C = list(np.power(10.0, np.arange(-10, 10)))\n",
        "cv_num = KFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "searchCV_l2 = linear_model.LogisticRegressionCV(\n",
        "          Cs= num_C\n",
        "          ,penalty='l2'\n",
        "          ,scoring='roc_auc'\n",
        "          ,cv=cv_num\n",
        "          ,random_state=42\n",
        "          ,max_iter=10000\n",
        "          ,fit_intercept=True\n",
        "          ,solver='newton-cg'\n",
        "          ,tol=10\n",
        "      )\n",
        "\n",
        "#searchCV.fit(X_train, y_train)\n",
        "searchCV_l2.fit(X_train, y_train)\n",
        "print ('Max auc_roc for l2:', searchCV_l2.scores_[1].mean(axis=0).max())\n",
        "\n",
        "\n",
        "print(\"Parameters for l2 regularisations\")\n",
        "print(searchCV_l2.coef_)\n",
        "print(searchCV_l2.intercept_)\n",
        "print(searchCV_l2.scores_)\n",
        "\n",
        "\n",
        "#find predicted vallues\n",
        "y_pred_l2 = searchCV_l2.predict(X_test)\n",
        "\n",
        "\n",
        "#Find predicted probabilities\n",
        "y_pred_probs_l2 = searchCV_l2.predict_proba(X_test)[:,1]\n",
        "\n",
        "\n",
        "# Accuaracy of L2/L1 models\n",
        "Accuracy_l2 = metrics.accuracy_score(y_pred=y_pred_l2, y_true=y_test)\n",
        "\n",
        "\n",
        "print(\"Accuarcy of Logistic model with l2 regularisation : {0}\".format(Accuracy_l2))\n",
        "\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "l2_roc_value = roc_auc_score(y_test, y_pred_probs_l2)\n",
        "print(\"l2 roc_value: {0}\" .format(l2_roc_value))\n",
        "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred_probs_l2)\n",
        "threshold = thresholds[np.argmax(tpr-fpr)]\n",
        "print(\"l2 threshold: {0}\".format(threshold))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "8ebL2svna4gt",
        "outputId": "b1daf854-9294-4c23-b061-7f685ac74989"
      },
      "outputs": [],
      "source": [
        "searchCV_l2.coef_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XOSjZTvuiKju"
      },
      "outputs": [],
      "source": [
        "coefficients = pd.concat([pd.DataFrame(X.columns),pd.DataFrame(np.transpose(searchCV_l2.coef_))], axis = 1)\n",
        "coefficients.columns = ['Feature','Importance Coefficient']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EDdjAJqPm0ae"
      },
      "outputs": [],
      "source": [
        "coefficients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "id": "RpEa4e1SiOhg",
        "outputId": "97b85057-0a6f-4b31-d0d1-526d23734fca"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20,5))\n",
        "sns.barplot(x='Feature', y='Importance Coefficient', data=coefficients)\n",
        "plt.title(\"Logistic Regression with L2 Regularisation Feature Importance\", fontsize=18)\n",
        "\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62TKNFjwhRTj"
      },
      "source": [
        "**Its is evident that V2,V4, V11 has + ve imporatnce whereas V14, V12, V10 seems to have -ve impact on the predictaions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UD0QrXvV3ZyZ"
      },
      "source": [
        "#As the models Oversampling data, take significantly longer time to run.\n",
        "We will try with undersampling methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install imblearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CqEoVNQ0akUu"
      },
      "outputs": [],
      "source": [
        "# Undersampling\n",
        "\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "#Define Oversampler\n",
        "RUS = RandomUnderSampler(sampling_strategy=0.5)\n",
        "# fit and apply the transform\n",
        "X_Under, y_Under = RUS.fit_resample(X_train, y_train)\n",
        "#Create Dataframe\n",
        "X_Under = pd.DataFrame(data=X_Under,   columns=cols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1o6dsersaxRN",
        "outputId": "6d50720d-30ee-40c8-dc14-b338dd1b9368"
      },
      "outputs": [],
      "source": [
        "#Run Logistic Regression with L1 And L2 Regularisation\n",
        "print(\"Logistic Regression with L1 And L2 Regularisation\")\n",
        "start_time = time.time()\n",
        "df_Results = buildAndRunLogisticModels(df_Results,\"Random Undersampling\", X_Under, y_Under , X_test, y_test)\n",
        "print(\"Time Taken by Model: --- %s seconds ---\" % (time.time() - start_time))\n",
        "print('-'*80 )\n",
        "#Run KNN Model\n",
        "print(\"KNN Model\")\n",
        "start_time = time.time()\n",
        "df_Results = buildAndRunKNNModels(df_Results,\"Random Undersampling\",X_Under, y_Under , X_test, y_test)\n",
        "print(\"Time Taken by Model: --- %s seconds ---\" % (time.time() - start_time))\n",
        "print('-'*80 )\n",
        "#Run Decision Tree Models with  'gini' & 'entropy' criteria\n",
        "print(\"Decision Tree Models with  'gini' & 'entropy' criteria\")\n",
        "start_time = time.time()\n",
        "df_Results = buildAndRunTreeModels(df_Results, \"Random Undersampling\",X_Under, y_Under , X_test, y_test)\n",
        "print(\"Time Taken by Model: --- %s seconds ---\" % (time.time() - start_time))\n",
        "print('-'*80 )\n",
        "#Run Random Forest Model\n",
        "print(\"Random Forest Model\")\n",
        "start_time = time.time()\n",
        "df_Results = buildAndRunRandomForestModels(df_Results, \"Random Undersampling\",X_Under, y_Under , X_test, y_test)\n",
        "print(\"Time Taken by Model: --- %s seconds ---\" % (time.time() - start_time))\n",
        "print('-'*80 )\n",
        "#Run XGBoost Model\n",
        "print(\"XGBoost Model\")\n",
        "start_time = time.time()\n",
        "df_Results = buildAndRunXGBoostModels(df_Results, \"Random Undersampling\",X_Under, y_Under , X_test, y_test)\n",
        "print(\"Time Taken by Model: --- %s seconds ---\" % (time.time() - start_time))\n",
        "print('-'*80 )\n",
        "#Run SVM Model with Sigmoid Kernel\n",
        "print(\"SVM Model with Sigmoid Kernel\")\n",
        "start_time = time.time()\n",
        "df_Results = buildAndRunSVMModels(df_Results, \"Random Undersampling\",X_Under, y_Under , X_test, y_test)\n",
        "print(\"Time Taken by Model: --- %s seconds ---\" % (time.time() - start_time))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yu26TugNCoHB",
        "outputId": "054b68e8-51e0-4214-f963-dbf3eeeaa11f"
      },
      "outputs": [],
      "source": [
        "df_Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGJirlk0C3Db"
      },
      "source": [
        "**It seems Undersampling has impoved the XGBoost Results**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFtYnW09gHeU"
      },
      "source": [
        "## Model building with balancing Classes\n",
        "\n",
        "##### Perform class balancing with :\n",
        "- Random Oversampling\n",
        "- SMOTE\n",
        "- ADASYN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avMxCc1sk7_Z"
      },
      "source": [
        "# Oversampling with RandomOverSampler and StratifiedKFold Cross Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YH3CG-P3mWUP"
      },
      "source": [
        "**We will use Random Oversampling method to handle the class imbalance**\n",
        "\n",
        "1. First we will display class distibution with and without the Random Oversampling.\n",
        "\n",
        "2. Then We will use the oversampled with StratifiedKFold cross validation method to genearte Train And test datasets.\n",
        "\n",
        "Once we have train and test dataset we will feed the data to below models:\n",
        "1. Logistic Regression with L2 Regularisation\n",
        "2. Logistic Regression with L1 Regularisation\n",
        "3. KNN\n",
        "4. Decision tree model with Gini criteria\n",
        "5. Decision tree model with Entropy criteria\n",
        "6. Random Forest\n",
        "7. XGBoost\n",
        "\n",
        "3. We did try SVM (support vector Machine) model , but due to extensive processive power requirement we avoided useing the model.\n",
        "\n",
        "4. Once we get results for above model, we will compare the results and select model which provided best results for the Random oversampling techinique"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "bxgJWJxb-AFR",
        "outputId": "3ebfc536-4df2-4a75-9917-935982e3db5f"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "#Define Oversampler\n",
        "ROS = RandomOverSampler(sampling_strategy=0.5)\n",
        "# fit and apply the transform\n",
        "X_over, y_over = ROS.fit_resample(X_train, y_train)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "id": "JFklfwuhVSZA",
        "outputId": "0a816a0a-a2aa-458e-8ccf-cd0808a7bacf"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5, random_state=None)\n",
        "\n",
        "for fold, (train_index, test_index) in enumerate(skf.split(X,y), 1):\n",
        "    X_train = X.loc[train_index]\n",
        "    y_train = y.loc[train_index]\n",
        "    X_test = X.loc[test_index]\n",
        "    y_test = y.loc[test_index]\n",
        "    ROS = RandomOverSampler(sampling_strategy=0.5)\n",
        "    X_over, y_over= ROS.fit_resample(X_train, y_train)\n",
        "\n",
        "#Create Dataframe for X_over\n",
        "X_over = pd.DataFrame(data=X_over,   columns=cols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dCkUq9vPkn-E",
        "outputId": "2b23bb68-29a6-4a21-a278-a108fcff0abb"
      },
      "outputs": [],
      "source": [
        "Data_Imbalance_Handiling\t = \"Random Oversampling with StratifiedKFold CV \"\n",
        "#Run Logistic Regression with L1 And L2 Regularisation\n",
        "print(\"Logistic Regression with L1 And L2 Regularisation\")\n",
        "start_time = time.time()\n",
        "df_Results = buildAndRunLogisticModels(df_Results , Data_Imbalance_Handiling , X_over, y_over, X_test, y_test)\n",
        "print(\"Time Taken by Model: --- %s seconds ---\" % (time.time() - start_time))\n",
        "print('-'*80 )\n",
        "#Run KNN Model\n",
        "print(\"KNN Model\")\n",
        "start_time = time.time()\n",
        "df_Results = buildAndRunKNNModels(df_Results , Data_Imbalance_Handiling,X_over, y_over, X_test, y_test)\n",
        "print(\"Time Taken by Model: --- %s seconds ---\" % (time.time() - start_time))\n",
        "print('-'*80 )\n",
        "#Run Decision Tree Models with  'gini' & 'entropy' criteria\n",
        "print(\"Decision Tree Models with  'gini' & 'entropy' criteria\")\n",
        "start_time = time.time()\n",
        "df_Results = buildAndRunTreeModels(df_Results , Data_Imbalance_Handiling,X_over, y_over, X_test, y_test)\n",
        "print(\"Time Taken by Model: --- %s seconds ---\" % (time.time() - start_time))\n",
        "print('-'*80 )\n",
        "#Run Random Forest Model\n",
        "print(\"Random Forest Model\")\n",
        "start_time = time.time()\n",
        "df_Results = buildAndRunRandomForestModels(df_Results , Data_Imbalance_Handiling,X_over, y_over, X_test, y_test)\n",
        "print(\"Time Taken by Model: --- %s seconds ---\" % (time.time() - start_time))\n",
        "print('-'*80 )\n",
        "#Run XGBoost Model\n",
        "print(\"XGBoost Model\")\n",
        "start_time = time.time()\n",
        "df_Results = buildAndRunXGBoostModels(df_Results , Data_Imbalance_Handiling,X_over, y_over, X_test, y_test)\n",
        "print(\"Time Taken by Model: --- %s seconds ---\" % (time.time() - start_time))\n",
        "print('-'*80 )\n",
        "#Run SVM Model with Sigmoid Kernel\n",
        "#print(\"SVM Model with Sigmoid Kernel\")\n",
        "#start_time = time.time()\n",
        "#df_Results = buildAndRunSVMModels(df_Results , Data_Imbalance_Handiling,X_over, y_over, X_test, y_test)\n",
        "#print(\"Time Taken by Model: --- %s seconds ---\" % (time.time() - start_time))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zk_8-lw7c25E",
        "outputId": "8cfde1ff-0662-40a6-a125-6eb3ded7deb0"
      },
      "outputs": [],
      "source": [
        "df_Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKsz9030lnT6"
      },
      "source": [
        "## Results for Random Oversampling:\n",
        "\n",
        "**Random Oversampling seems to have +ve change in prediction for XGBoost**\n",
        "Looking at Accuracy and ROC value we have XGBoost which has provided best results for SMOTE oversampling technique"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVJOmI2YgHei"
      },
      "source": [
        "### Similarly explore other algorithms on balanced dataset by building models like:\n",
        "- KNN\n",
        "- SVM\n",
        "- Decision Tree\n",
        "- Random Forest\n",
        "- XGBoost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrVrqvUa4pVj"
      },
      "source": [
        "# Oversampling with  SMOTE Oversamplng"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsOIWYB_kz4k"
      },
      "source": [
        "**We will use SMOTE Oversampling method to handle the class imbalance**\n",
        "\n",
        "1. First we will display class distibution with and without the SMOTE Oversampling.\n",
        "\n",
        "2. Then We will use the oversampled with StratifiedKFold cross validation method to genearte Train And test datasets.\n",
        "\n",
        "Once we have train and test dataset we will feed the data to below models:\n",
        "1. Logistic Regression with L2 Regularisation\n",
        "2. Logistic Regression with L1 Regularisation\n",
        "3. KNN\n",
        "4. Decision tree model with Gini criteria\n",
        "5. Decision tree model with Entropy criteria\n",
        "6. Random Forest\n",
        "7. XGBoost\n",
        "\n",
        "3. We did try SVM (support vector Machine) model , but due to extensive processive power requirement we avoided useing the model.\n",
        "\n",
        "4. Once we get results for above model, we will compare the results and select model which provided best results for the SMOTE oversampling techinique"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQ3dlD3lgHen"
      },
      "source": [
        "### Print the class distribution after applying SMOTE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Apply SMOTE\n",
        "smote = SMOTE(random_state=0)\n",
        "X_train_Smote, y_train_Smote = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Convert to DataFrame\n",
        "X_train_Smote = pd.DataFrame(X_train_Smote, columns=X_train.columns)\n",
        "\n",
        "# Extract synthetic samples\n",
        "num_original_samples = X_train.shape[0]\n",
        "X_train_smote_1 = X_train_Smote.iloc[num_original_samples:].to_numpy()\n",
        "\n",
        "# Extract actual class samples\n",
        "X_train_array = X_train.to_numpy()\n",
        "y_train_array = y_train.to_numpy()\n",
        "\n",
        "X_train_1 = X_train_array[y_train_array == 1]\n",
        "X_train_0 = X_train_array[y_train_array == 0]\n",
        "\n",
        "# Visualization\n",
        "plt.rcParams['figure.figsize'] = [12, 12]\n",
        "fig, axs = plt.subplots(3, 1, figsize=(8, 15))\n",
        "\n",
        "# Plot 1: Original Class-1 Samples\n",
        "axs[0].scatter(X_train_1[:, 0], X_train_1[:, 1], label='Actual Class-1', alpha=0.6)\n",
        "axs[0].legend()\n",
        "axs[0].set_title('Original Class-1 Samples')\n",
        "\n",
        "# Plot 2: Class-1 with Synthetic SMOTE Samples\n",
        "axs[1].scatter(X_train_1[:, 0], X_train_1[:, 1], label='Actual Class-1', alpha=0.6)\n",
        "axs[1].scatter(X_train_smote_1[:X_train_1.shape[0], 0], \n",
        "               X_train_smote_1[:X_train_1.shape[0], 1], \n",
        "               label='Synthetic Class-1 (SMOTE)', alpha=0.6, marker='x')\n",
        "axs[1].legend()\n",
        "axs[1].set_title('SMOTE Synthetic Samples for Class-1')\n",
        "\n",
        "# Plot 3: Class-1 vs. Class-0\n",
        "axs[2].scatter(X_train_1[:, 0], X_train_1[:, 1], label='Actual Class-1', alpha=0.6)\n",
        "axs[2].scatter(X_train_0[:X_train_1.shape[0], 0], \n",
        "               X_train_0[:X_train_1.shape[0], 1], \n",
        "               label='Actual Class-0', alpha=0.6, marker='s')\n",
        "axs[2].legend()\n",
        "axs[2].set_title('Class-1 vs Class-0 Distribution')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zoV48NbQeKHJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "from imblearn import over_sampling\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5, random_state=None)\n",
        "\n",
        "for fold, (train_index, test_index) in enumerate(skf.split(X,y), 1):\n",
        "    X_train = X.loc[train_index]\n",
        "    y_train = y.loc[train_index]\n",
        "    X_test = X.loc[test_index]\n",
        "    y_test = y.loc[test_index]\n",
        "    SMOTE = over_sampling.SMOTE(random_state=0)\n",
        "    X_train_Smote, y_train_Smote= SMOTE.fit_resample(X_train, y_train)\n",
        "\n",
        "#Create Dataframe for X_over\n",
        "X_train_Smote = pd.DataFrame(data=X_train_Smote,   columns=cols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "59FAfGMy3uPO",
        "outputId": "990bd5b0-9f46-4143-a3fb-0eb022660b7c"
      },
      "outputs": [],
      "source": [
        "Data_Imbalance_Handiling\t = \"SMOTE Oversampling with StratifiedKFold CV \"\n",
        "#Run Logistic Regression with L1 And L2 Regularisation\n",
        "print(\"Logistic Regression with L1 And L2 Regularisation\")\n",
        "start_time = time.time()\n",
        "df_Results = buildAndRunLogisticModels(df_Results, Data_Imbalance_Handiling, X_train_Smote, y_train_Smote , X_test, y_test)\n",
        "print(\"Time Taken by Model: --- %s seconds ---\" % (time.time() - start_time))\n",
        "print('-'*80 )\n",
        "#Run KNN Model\n",
        "print(\"KNN Model\")\n",
        "start_time = time.time()\n",
        "df_Results = buildAndRunKNNModels(df_Results, Data_Imbalance_Handiling, X_train_Smote, y_train_Smote , X_test, y_test)\n",
        "print(\"Time Taken by Model: --- %s seconds ---\" % (time.time() - start_time))\n",
        "print('-'*80 )\n",
        "#Run Decision Tree Models with  'gini' & 'entropy' criteria\n",
        "print(\"Decision Tree Models with  'gini' & 'entropy' criteria\")\n",
        "start_time = time.time()\n",
        "df_Results = buildAndRunTreeModels(df_Results, Data_Imbalance_Handiling, X_train_Smote, y_train_Smote , X_test, y_test)\n",
        "print(\"Time Taken by Model: --- %s seconds ---\" % (time.time() - start_time))\n",
        "print('-'*80 )\n",
        "#Run Random Forest Model\n",
        "print(\"Random Forest Model\")\n",
        "start_time = time.time()\n",
        "df_Results = buildAndRunRandomForestModels(df_Results, Data_Imbalance_Handiling, X_train_Smote, y_train_Smote , X_test, y_test)\n",
        "print(\"Time Taken by Model: --- %s seconds ---\" % (time.time() - start_time))\n",
        "print('-'*80 )\n",
        "#Run XGBoost Model\n",
        "print(\"XGBoost Model\")\n",
        "start_time = time.time()\n",
        "df_Results = buildAndRunXGBoostModels(df_Results, Data_Imbalance_Handiling, X_train_Smote, y_train_Smote , X_test, y_test)\n",
        "print(\"Time Taken by Model: --- %s seconds ---\" % (time.time() - start_time))\n",
        "print('-'*80 )\n",
        "#Run SVM Model with Sigmoid Kernel\n",
        "#print(\"SVM Model with Sigmoid Kernel\")\n",
        "#start_time = time.time()\n",
        "#df_Results = buildAndRunSVMModels(df_Results, Data_Imbalance_Handiling, X_train_Smote, y_train_Smote , X_test, y_test)\n",
        "#print(\"Time Taken by Model: --- %s seconds ---\" % (time.time() - start_time))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Qt3_uZfgHew"
      },
      "source": [
        "##### Build models on other algorithms to see the better performing on SMOTE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3Sl3TxAWkILE",
        "outputId": "f34c0d04-4f30-41ca-c42f-1ecb23c0442d"
      },
      "outputs": [],
      "source": [
        "df_Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8ecSlutk7nh"
      },
      "source": [
        "## Results for SMOTE Oversampling:\n",
        "\n",
        "Looking at Accuracy and ROC value we have XGBoost which has provided best results for SMOTE oversampling technique"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nF7rqQkUjrxW"
      },
      "source": [
        "# Oversampling with ADASYN Oversampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgUngp16jF9A"
      },
      "source": [
        "**We will use ADASYN Oversampling method to handle the class imbalance**\n",
        "\n",
        "1. First we will display class distibution with and without the ADASYN Oversampling.\n",
        "\n",
        "2. Then We will use the oversampled with StratifiedKFold cross validation method to genearte Train And test datasets.\n",
        "\n",
        "Once we have train and test dataset we will feed the data to below models:\n",
        "1. Logistic Regression with L2 Regularisation\n",
        "2. Logistic Regression with L1 Regularisation\n",
        "3. KNN\n",
        "4. Decision tree model with Gini criteria\n",
        "5. Decision tree model with Entropy criteria\n",
        "6. Random Forest\n",
        "7. XGBoost\n",
        "\n",
        "3. We did try SVM (support vector Machine) model , but due to extensive processive power requirement we avoided useing the model.\n",
        "\n",
        "4. Once we get results for above model, we will compare the results and select model which provided best results for the oversampling techinique\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "AATl2glygHez",
        "outputId": "72e67ef3-e43b-47a4-9909-2c535fc632e7"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from imblearn.over_sampling import ADASYN\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Apply ADASYN\n",
        "adasyn = ADASYN(random_state=0)\n",
        "X_train_ADASYN, y_train_ADASYN = adasyn.fit_resample(X_train, y_train)\n",
        "\n",
        "# Convert to DataFrame\n",
        "X_train_ADASYN = pd.DataFrame(X_train_ADASYN, columns=X_train.columns)\n",
        "\n",
        "# Extract synthetic samples\n",
        "num_original_samples = X_train.shape[0]\n",
        "X_train_adasyn_1 = X_train_ADASYN.iloc[num_original_samples:].to_numpy()\n",
        "\n",
        "# Extract actual class samples\n",
        "X_train_array = X_train.to_numpy()\n",
        "y_train_array = y_train.to_numpy()\n",
        "\n",
        "X_train_1 = X_train_array[y_train_array == 1]\n",
        "X_train_0 = X_train_array[y_train_array == 0]\n",
        "\n",
        "# Visualization\n",
        "plt.rcParams['figure.figsize'] = [12, 12]\n",
        "fig, axs = plt.subplots(3, 1, figsize=(8, 15))\n",
        "\n",
        "# Plot 1: Original Class-1 Samples\n",
        "axs[0].scatter(X_train_1[:, 0], X_train_1[:, 1], label='Actual Class-1', alpha=0.6)\n",
        "axs[0].legend()\n",
        "axs[0].set_title('Original Class-1 Samples')\n",
        "\n",
        "# Plot 2: Class-1 with Synthetic ADASYN Samples\n",
        "axs[1].scatter(X_train_1[:, 0], X_train_1[:, 1], label='Actual Class-1', alpha=0.6)\n",
        "axs[1].scatter(X_train_adasyn_1[:X_train_1.shape[0], 0], \n",
        "               X_train_adasyn_1[:X_train_1.shape[0], 1], \n",
        "               label='Synthetic Class-1 (ADASYN)', alpha=0.6, marker='x')\n",
        "axs[1].legend()\n",
        "axs[1].set_title('ADASYN Synthetic Samples for Class-1')\n",
        "\n",
        "# Plot 3: Class-1 vs. Class-0\n",
        "axs[2].scatter(X_train_1[:, 0], X_train_1[:, 1], label='Actual Class-1', alpha=0.6)\n",
        "axs[2].scatter(X_train_0[:X_train_1.shape[0], 0], \n",
        "               X_train_0[:X_train_1.shape[0], 1], \n",
        "               label='Actual Class-0', alpha=0.6, marker='s')\n",
        "axs[2].legend()\n",
        "axs[2].set_title('Class-1 vs Class-0 Distribution')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6h-Q3_7kWz4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from imblearn.over_sampling import ADASYN\n",
        "\n",
        "# Stratified K-Fold with ADASYN\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Placeholder for resampled datasets (optional)\n",
        "resampled_data = []\n",
        "\n",
        "for fold, (train_index, test_index) in enumerate(skf.split(X, y), 1):\n",
        "    # Splitting dataset\n",
        "    X_train = X.iloc[train_index]\n",
        "    y_train = y.iloc[train_index]\n",
        "    X_test = X.iloc[test_index]\n",
        "    y_test = y.iloc[test_index]\n",
        "    \n",
        "    # Apply ADASYN for oversampling\n",
        "    adasyn = ADASYN(random_state=0)\n",
        "    X_train_ADASYN, y_train_ADASYN = adasyn.fit_resample(X_train, y_train)\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    X_train_ADASYN = pd.DataFrame(X_train_ADASYN, columns=X.columns)\n",
        "    \n",
        "    # (Optional) Store results\n",
        "    resampled_data.append((X_train_ADASYN, y_train_ADASYN))\n",
        "\n",
        "    print(f\"Fold {fold}: Training samples before ADASYN = {X_train.shape[0]}, after = {X_train_ADASYN.shape[0]}\")\n",
        "\n",
        "# If you want access to each resampled dataset, they are stored in resampled_data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sbpk1TZdgHe9"
      },
      "source": [
        "##### Build models on other algorithms to see the better performing on ADASYN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0ZFaib9A3uIH",
        "outputId": "14093293-e5f1-47fe-8d25-a978a2c74d92"
      },
      "outputs": [],
      "source": [
        "Data_Imbalance_Handiling\t = \"ADASYN Oversampling with StratifiedKFold CV \"\n",
        "#Run Logistic Regression with L1 And L2 Regularisation\n",
        "print(\"Logistic Regression with L1 And L2 Regularisation\")\n",
        "start_time = time.time()\n",
        "df_Results = buildAndRunLogisticModels(df_Results, Data_Imbalance_Handiling, X_train_ADASYN, y_train_ADASYN , X_test, y_test)\n",
        "print(\"Time Taken by Model: --- %s seconds ---\" % (time.time() - start_time))\n",
        "print('-'*80 )\n",
        "#Run KNN Model\n",
        "print(\"KNN Model\")\n",
        "start_time = time.time()\n",
        "df_Results = buildAndRunKNNModels(df_Results, Data_Imbalance_Handiling,X_train_ADASYN, y_train_ADASYN , X_test, y_test)\n",
        "print(\"Time Taken by Model: --- %s seconds ---\" % (time.time() - start_time))\n",
        "print('-'*80 )\n",
        "#Run Decision Tree Models with  'gini' & 'entropy' criteria\n",
        "print(\"Decision Tree Models with  'gini' & 'entropy' criteria\")\n",
        "start_time = time.time()\n",
        "df_Results = buildAndRunTreeModels(df_Results, Data_Imbalance_Handiling,X_train_ADASYN, y_train_ADASYN , X_test, y_test)\n",
        "print(\"Time Taken by Model: --- %s seconds ---\" % (time.time() - start_time))\n",
        "print('-'*80 )\n",
        "#Run Random Forest Model\n",
        "print(\"Random Forest Model\")\n",
        "start_time = time.time()\n",
        "df_Results = buildAndRunRandomForestModels(df_Results, Data_Imbalance_Handiling,X_train_ADASYN, y_train_ADASYN , X_test, y_test)\n",
        "print(\"Time Taken by Model: --- %s seconds ---\" % (time.time() - start_time))\n",
        "print('-'*80 )\n",
        "#Run XGBoost Model\n",
        "print(\"XGBoost Model\")\n",
        "start_time = time.time()\n",
        "df_Results = buildAndRunXGBoostModels(df_Results, Data_Imbalance_Handiling,X_train_ADASYN, y_train_ADASYN , X_test, y_test)\n",
        "print(\"Time Taken by Model: --- %s seconds ---\" % (time.time() - start_time))\n",
        "print('-'*80 )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tBab0f8LsUxn",
        "outputId": "35002c35-0690-4106-b155-84f37d542581"
      },
      "outputs": [],
      "source": [
        "df_Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNFPx-pLlQbT"
      },
      "source": [
        "## Results for ADASYN  Oversampling:\n",
        "\n",
        "Looking at Accuracy and ROC value we have XGBoost which has provided best results for ADASYN oversampling technique"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVl6tYp3ijpP"
      },
      "source": [
        "## Overall conclusion after running models on Oversampled data:\n",
        "\n",
        "Looking at above results it seems XGBOOST model with Random Oversampling with StratifiedKFold CV has provided best results. So we can try to tune the hyperparameters of this model to get best results\n",
        "\n",
        "\n",
        "## But looking at the results \tLogistic Regression with L2 Regularisation with RepeatedKFold Cross Validation has been provided best results without any oversampling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjFN9_mpvRdy"
      },
      "source": [
        "#Parameter Tuning for Final Model by Handling class imbalance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-MHg0j3t_Iw"
      },
      "outputs": [],
      "source": [
        "#Evaluate XGboost model\n",
        "from xgboost import XGBClassifier\n",
        "# fit model no training data\n",
        "XGBmodel = XGBClassifier(random_state=42)\n",
        "XGBmodel.fit(X_over, y_over)\n",
        "\n",
        "XGB_test_score = XGBmodel.score(X_test, y_test)\n",
        "print('Model Accuracy: {0}'.format(XGB_test_score))\n",
        "\n",
        "# Probabilities for each class\n",
        "XGB_probs = XGBmodel.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate roc auc\n",
        "XGB_roc_value = roc_auc_score(y_test, XGB_probs)\n",
        "\n",
        "print(\"XGboost roc_value: {0}\" .format(XGB_roc_value))\n",
        "fpr, tpr, thresholds = metrics.roc_curve(y_test, XGB_probs)\n",
        "threshold = thresholds[np.argmax(tpr-fpr)]\n",
        "print(\"XGBoost threshold: {0}\".format(threshold))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z3c48wSPt_Vn"
      },
      "outputs": [],
      "source": [
        "#Evaluate XGboost model\n",
        "from xgboost import XGBClassifier\n",
        "# fit model no training data\n",
        "XGBmodel = XGBClassifier(random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WHwD1jY3uiey",
        "outputId": "1d12d18c-19e8-4982-82f1-97cb03bb7c60"
      },
      "outputs": [],
      "source": [
        "#Lets tune XGBoost Model for max_depth and min_child_weight\n",
        "from xgboost.sklearn import XGBClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "param_test = {\n",
        " 'max_depth':range(3,10,2),\n",
        " 'min_child_weight':range(1,6,2)\n",
        "}\n",
        "gsearch1 = GridSearchCV(estimator = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
        "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
        "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
        "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=-1,\n",
        "              nthread=None, objective='binary:logistic', random_state=42,\n",
        "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
        "              silent=None, subsample=1, verbosity=1),\n",
        " param_grid = param_test, scoring='roc_auc',n_jobs=4, cv=5)\n",
        "gsearch1.fit(X_over, y_over)\n",
        "gsearch1.cv_results_, gsearch1.best_params_, gsearch1.best_score_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "id": "tgPUhFrIuijI",
        "outputId": "253bd175-cb5f-4aa4-c851-633fc6f39b46"
      },
      "outputs": [],
      "source": [
        "#Lets tune XGBoost Model for n_estimators\n",
        "from xgboost.sklearn import XGBClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "param_test = {\n",
        " 'n_estimators':range(60,150,20)\n",
        "}\n",
        "gsearch1 = GridSearchCV(estimator = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
        "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
        "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
        "              min_child_weight=5, missing=None, n_estimators=100, n_jobs=-1,\n",
        "              nthread=None, objective='binary:logistic', random_state=42,\n",
        "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
        "              silent=None, subsample=1, verbosity=1),\n",
        " param_grid = param_test, scoring='roc_auc',n_jobs=-4, cv=5)\n",
        "gsearch1.fit(X_over, y_over)\n",
        "gsearch1.cv_results_, gsearch1.best_params_, gsearch1.best_score_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hBJ-hSeeuiqm",
        "outputId": "eb241c7a-139d-40dd-905e-ffbdd4849702"
      },
      "outputs": [],
      "source": [
        "# We will narror down the tunned parameters of max_depth ,  min_child_weight and n_estimators\n",
        "#Lets tune XGBoost Model for n_estimators\n",
        "from xgboost.sklearn import XGBClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "param_test = {\n",
        " 'n_estimators':[110,120,130],\n",
        "  'max_depth':[2,3,4],\n",
        " 'min_child_weight':[4,5,6]\n",
        "}\n",
        "gsearch1 = GridSearchCV(estimator = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
        "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
        "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
        "              min_child_weight=5, missing=None, n_estimators=120, n_jobs=-1,\n",
        "              nthread=None, objective='binary:logistic', random_state=42,\n",
        "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
        "              silent=None, subsample=1, verbosity=1),\n",
        " param_grid = param_test, scoring='roc_auc',n_jobs=4, cv=5)\n",
        "gsearch1.fit(X_over, y_over)\n",
        "gsearch1.cv_results_, gsearch1.best_params_, gsearch1.best_score_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "id": "IY8oy7hVuink",
        "outputId": "9ff1fb97-2802-4dda-c108-21ee1c511d63"
      },
      "outputs": [],
      "source": [
        "#With 'max_depth': 4, 'min_child_weight': 4, 'n_estimators': 130 parameters tuned above we will now check learning rate\n",
        "\n",
        "# We will narror down the tunned parameters of max_depth ,  min_child_weight and n_estimators\n",
        "#Lets tune XGBoost Model for n_estimators\n",
        "from xgboost.sklearn import XGBClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "param_test = {\n",
        " 'learning_rate':[0.05,0.1,0.125,0.15,0.2]\n",
        "}\n",
        "gsearch1 = GridSearchCV(estimator = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
        "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
        "              learning_rate=0.1, max_delta_step=0, max_depth=4,\n",
        "              min_child_weight=4, n_estimators=130, n_jobs=-1,\n",
        "              nthread=None, objective='binary:logistic', random_state=42,\n",
        "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
        "              silent=None, subsample=1, verbosity=1),\n",
        " param_grid = param_test, scoring='roc_auc',n_jobs=4, cv=5)\n",
        "gsearch1.fit(X_over, y_over)\n",
        "gsearch1.cv_results_, gsearch1.best_params_, gsearch1.best_score_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "id": "ZNrO0wXxxiwX",
        "outputId": "63cfaeb9-5c72-4d70-a498-5b9d7799b7f5"
      },
      "outputs": [],
      "source": [
        "#With 'max_depth': 4, 'min_child_weight': 4, 'n_estimators': 130 parameters tuned above we will now check learning rate\n",
        "\n",
        "# We will narror down the tunned parameters of max_depth ,  min_child_weight and n_estimators\n",
        "#Lets tune XGBoost Model for n_estimators\n",
        "from xgboost.sklearn import XGBClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "param_test = {\n",
        "'gamma':[i/10.0 for i in range(0,5)]\n",
        "}\n",
        "gsearch1 = GridSearchCV(estimator = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
        "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
        "              learning_rate=0.2, max_delta_step=0, max_depth=4,\n",
        "              min_child_weight=4, n_estimators=130, n_jobs=-1,\n",
        "              nthread=None, objective='binary:logistic', random_state=42,\n",
        "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
        "              subsample=1, verbosity=1),\n",
        " param_grid = param_test, scoring='roc_auc',n_jobs=4, cv=5)\n",
        "gsearch1.fit(X_over, y_over)\n",
        "gsearch1.cv_results_, gsearch1.best_params_, gsearch1.best_score_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "id": "845VzVzW7Pdp",
        "outputId": "de2bdca6-4de7-4d9c-d724-9d62af136261"
      },
      "outputs": [],
      "source": [
        "#Evaluate XGboost model\n",
        "from xgboost import XGBClassifier\n",
        "# fit model no training data\n",
        "XGBmodel = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
        "              colsample_bynode=1, colsample_bytree=1, gamma=0.1,\n",
        "              learning_rate=0.2, max_delta_step=0, max_depth=4,\n",
        "              min_child_weight=4, n_estimators=130, n_jobs=-1,\n",
        "              nthread=None, objective='binary:logistic', random_state=42,\n",
        "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
        "              subsample=1, verbosity=1)\n",
        "XGBmodel.fit(X_over, y_over)\n",
        "\n",
        "XGB_test_score = XGBmodel.score(X_test, y_test)\n",
        "print('Model Accuracy: {0}'.format(XGB_test_score))\n",
        "\n",
        "# Probabilities for each class\n",
        "XGB_probs = XGBmodel.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate roc auc\n",
        "XGB_roc_value = roc_auc_score(y_test, XGB_probs)\n",
        "\n",
        "print(\"XGboost roc_value: {0}\" .format(XGB_roc_value))\n",
        "fpr, tpr, thresholds = metrics.roc_curve(y_test, XGB_probs)\n",
        "threshold = thresholds[np.argmax(tpr-fpr)]\n",
        "print(\"XGBoost threshold: {0}\".format(threshold))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 836
        },
        "id": "ziYDsFPq8v3Y",
        "outputId": "c4954673-edef-4499-9f44-525b0a2c1ea9"
      },
      "outputs": [],
      "source": [
        "#With 'max_depth': 4, 'min_child_weight': 4, 'n_estimators': 130 , gamma: 0.1 parameters tuned above we will now check learning rate\n",
        "\n",
        "# We will narror down the tunned parameters of max_depth ,  min_child_weight and n_estimators\n",
        "#Lets tune XGBoost Model for n_estimators\n",
        "from xgboost.sklearn import XGBClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "param_test = {\n",
        " 'subsample':[i/10.0 for i in range(7,10)],\n",
        " 'colsample_bytree':[i/10.0 for i in range(7,10)]\n",
        "}\n",
        "\n",
        "gsearch1 = GridSearchCV(estimator = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
        "              colsample_bynode=1, colsample_bytree=1, gamma=0.1,\n",
        "              learning_rate=0.2, max_delta_step=0, max_depth=4,\n",
        "              min_child_weight=4, missing=None, n_estimators=130, n_jobs=1,\n",
        "              nthread=None, objective='binary:logistic', random_state=42,\n",
        "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
        "              silent=None, subsample=1, verbosity=1),\n",
        " param_grid = param_test, scoring='roc_auc',n_jobs=4, cv=5)\n",
        "gsearch1.fit(X_over, y_over)\n",
        "gsearch1.cv_results_, gsearch1.best_params_, gsearch1.best_score_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "id": "edNy47uYG9ki",
        "outputId": "9bf7738a-a3b7-4cea-f71e-ebdd592d7e78"
      },
      "outputs": [],
      "source": [
        "#Evaluate XGboost model\n",
        "from xgboost import XGBClassifier\n",
        "# fit model no training data\n",
        "XGBmodel = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=0.8,\n",
        "              colsample_bynode=1, colsample_bytree=1, gamma=0.1,\n",
        "              learning_rate=0.2, max_delta_step=0, max_depth=4,\n",
        "              min_child_weight=4, n_estimators=130, n_jobs=1,\n",
        "              nthread=None, objective='binary:logistic', random_state=42,\n",
        "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
        "              subsample=0.8, verbosity=1)\n",
        "XGBmodel.fit(X_over, y_over)\n",
        "\n",
        "XGB_test_score = XGBmodel.score(X_test, y_test)\n",
        "print('Model Accuracy: {0}'.format(XGB_test_score))\n",
        "\n",
        "# Probabilities for each class\n",
        "XGB_probs = XGBmodel.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate roc auc\n",
        "XGB_roc_value = roc_auc_score(y_test, XGB_probs)\n",
        "\n",
        "print(\"XGboost roc_value: {0}\" .format(XGB_roc_value))\n",
        "fpr, tpr, thresholds = metrics.roc_curve(y_test, XGB_probs)\n",
        "threshold = thresholds[np.argmax(tpr-fpr)]\n",
        "print(\"XGBoost threshold: {0}\".format(threshold))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1j29TFPHqMl"
      },
      "source": [
        "**As the roc value has dropped we will take not consider new values of colsample_bytree': 0.8, 'subsample': 0.8**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "id": "aNpSMA5xgHe_",
        "outputId": "eae52576-80f5-48c0-bb36-3d2f524eb041"
      },
      "outputs": [],
      "source": [
        "# perform the best oversampling method on X_train & y_train\n",
        "\n",
        "clf = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
        "              colsample_bynode=1, colsample_bytree=1, gamma=0.1,\n",
        "              learning_rate=0.2, max_delta_step=0, max_depth=4,\n",
        "              min_child_weight=4,  n_estimators=130, n_jobs=1,\n",
        "              nthread=None, objective='binary:logistic', random_state=42,\n",
        "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
        "               subsample=1, verbosity=1)\n",
        "clf.fit(X_over, y_over ) # fit on the balanced dataset\n",
        "XGB_test_score = clf.score(X_test, y_test)\n",
        "print('Model Accuracy: {0}'.format(XGB_test_score))\n",
        "\n",
        "# Probabilities for each class\n",
        "XGB_probs = clf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate roc auc\n",
        "XGB_roc_value = roc_auc_score(y_test, XGB_probs)\n",
        "\n",
        "print(\"XGboost roc_value: {0}\" .format(XGB_roc_value))\n",
        "fpr, tpr, thresholds = metrics.roc_curve(y_test, XGB_probs)\n",
        "threshold = thresholds[np.argmax(tpr-fpr)]\n",
        "print(\"XGBoost threshold: {0}\".format(threshold))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9oJYPD-gHfD"
      },
      "source": [
        "### Print the important features of the best model to understand the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "42b6sHzRgHfE",
        "outputId": "df713d19-b9bf-44e1-e34d-a765f5800d25"
      },
      "outputs": [],
      "source": [
        "var_imp = []\n",
        "for i in clf.feature_importances_:\n",
        "    var_imp.append(i)\n",
        "print('Top var =', var_imp.index(np.sort(clf.feature_importances_)[-1])+1)\n",
        "print('2nd Top var =', var_imp.index(np.sort(clf.feature_importances_)[-2])+1)\n",
        "print('3rd Top var =', var_imp.index(np.sort(clf.feature_importances_)[-3])+1)\n",
        "\n",
        "# Variable on Index-13 and Index-9 seems to be the top 2 variables\n",
        "top_var_index = var_imp.index(np.sort(clf.feature_importances_)[-1])\n",
        "second_top_var_index = var_imp.index(np.sort(clf.feature_importances_)[-2])\n",
        "\n",
        "X_train_1 = X_train.to_numpy()[np.where(y_train==1.0)]\n",
        "X_train_0 = X_train.to_numpy()[np.where(y_train==0.0)]\n",
        "\n",
        "np.random.shuffle(X_train_0)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = [20, 20]\n",
        "\n",
        "plt.scatter(X_train_1[:, top_var_index], X_train_1[:, second_top_var_index], label='Actual Class-1 Examples')\n",
        "plt.scatter(X_train_0[:X_train_1.shape[0], top_var_index], X_train_0[:X_train_1.shape[0], second_top_var_index],\n",
        "            label='Actual Class-0 Examples')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKr9RBKvgHfJ"
      },
      "outputs": [],
      "source": [
        "#### Print the FPR,TPR & select the best threshold from the roc curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "id": "oYyS8GDJgHfM",
        "outputId": "fbfa14f2-cc44-48c4-d089-024c2490fb72"
      },
      "outputs": [],
      "source": [
        "# Calculate roc auc\n",
        "XGB_roc_value = roc_auc_score(y_test, XGB_probs)\n",
        "\n",
        "print(\"XGboost roc_value: {0}\" .format(XGB_roc_value))\n",
        "fpr, tpr, thresholds = metrics.roc_curve(y_test, XGB_probs)\n",
        "threshold = thresholds[np.argmax(tpr-fpr)]\n",
        "print(\"XGBoost threshold: {0}\".format(threshold))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0MvrQ3-s_MQ"
      },
      "source": [
        "## Model Selection:\n",
        "## Overall conclusion after running models on Oversampled data:\n",
        "\n",
        "Looking at above results it seems XGBOOST model with Random Oversampling with StratifiedKFold CV has provided best results. So we can try to tune the hyperparameters of this model to get best results\n",
        "\n",
        "We have selected XGBOOST model with Random Oversampling and StratifiedKFold CV\n",
        "\n",
        "\n",
        "**Model Accuracy: 0.9997191060550201**\n",
        "\n",
        "**XGboost roc_value: 0.9975323791124173**\n",
        "\n",
        "**XGBoost threshold: 0.0108169075101614**\n",
        "\n",
        "### We also noticed by looking at the results \tLogistic Regression with L2 Regularisation with RepeatedKFold Cross Validation has been provided best results without any oversampling.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
